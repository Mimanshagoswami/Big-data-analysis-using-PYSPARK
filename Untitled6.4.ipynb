{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1903c2f-5468-4107-85d8-23a3927d0762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"from pyspark.sql import SparkSession\\n\",\n",
    "    \"from pyspark import StorageLevel\\n\",\n",
    "    \"from pyspark.sql import functions as f\\n\",\n",
    "    \"\\n\",\n",
    "    \"spark = (\\n\",\n",
    "    \"    SparkSession.builder.appName(\\\"ModelTraining\\\")\\n\",\n",
    "    \"    .config(\\\"spark.executor.memory\\\", \\\"4g\\\")\\n\",\n",
    "    \"    .getOrCreate()\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"pd.options.display.max_columns = None\\n\",\n",
    "    \"pd.options.display.max_rows = 30\\n\",\n",
    "    \"pd.options.display.max_colwidth = 150\\n\",\n",
    "    \"\\n\",\n",
    "    \"schema = \\\"polarity FLOAT, id LONG, date_time TIMESTAMP, query STRING, user STRING, text STRING\\\"\\n\",\n",
    "    \"timestampformat = \\\"EEE MMM dd HH:mm:ss zzz yyyy\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"IN_PATH = \\\"/home/jovyan/data-sets/sentiment-140-training-data/CLEAN\\\"\\n\",\n",
    "    \"MODEL_PATH = \\\"/home/jovyan/data-sets/sentiment-140-training-data/MODEL\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"spark_reader = spark.read.schema(schema)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_clean = spark_reader.parquet(IN_PATH).cache()\\n\",\n",
    "    \"df_clean = (\\n\",\n",
    "    \"    df_clean\\n\",\n",
    "    \"    # Remove all numbers\\n\",\n",
    "    \"    .withColumn(\\\"text\\\", f.regexp_replace(f.col(\\\"text\\\"), \\\"[^a-zA-Z']\\\", \\\" \\\"))\\n\",\n",
    "    \"    # Remove all double/multiple spaces\\n\",\n",
    "    \"    .withColumn(\\\"text\\\", f.regexp_replace(f.col(\\\"text\\\"), \\\" +\\\", \\\" \\\"))\\n\",\n",
    "    \"    # Remove leading and trailing whitespaces\\n\",\n",
    "    \"    .withColumn(\\\"text\\\", f.trim(f.col(\\\"text\\\")))\\n\",\n",
    "    \"    # Ensure we don't end up with empty rows\\n\",\n",
    "    \"    .filter(\\\"text != ''\\\")\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"data = df_clean.select(\\\"text\\\", \\\"polarity\\\").coalesce(3).cache()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"+--------------------+--------+\\n\",\n",
    "      \"|                text|polarity|\\n\",\n",
    "      \"+--------------------+--------+\\n\",\n",
    "      \"|I just havn't bee...|     0.0|\\n\",\n",
    "      \"|My drummer got in...|     0.0|\\n\",\n",
    "      \"|              Cramps|     0.0|\\n\",\n",
    "      \"|watching I am sam...|     0.0|\\n\",\n",
    "      \"|      My tummy hurts|     0.0|\\n\",\n",
    "      \"|damn times faster...|     0.0|\\n\",\n",
    "      \"|Holy shit it's su...|     0.0|\\n\",\n",
    "      \"|so glad its sunny...|     0.0|\\n\",\n",
    "      \"|is cold still has...|     0.0|\\n\",\n",
    "      \"|pardon the french...|     0.0|\\n\",\n",
    "      \"|Our human mom jus...|     0.0|\\n\",\n",
    "      \"|I think waayy to ...|     0.0|\\n\",\n",
    "      \"|I can't make it t...|     0.0|\\n\",\n",
    "      \"|It's too sunny an...|     0.0|\\n\",\n",
    "      \"|isn't sleeping to...|     0.0|\\n\",\n",
    "      \"|Getting ready to ...|     0.0|\\n\",\n",
    "      \"|Just paid for the...|     0.0|\\n\",\n",
    "      \"|I know the feelin...|     0.0|\\n\",\n",
    "      \"|    I miss TweetDeck|     0.0|\\n\",\n",
    "      \"|why are there NO ...|     0.0|\\n\",\n",
    "      \"+--------------------+--------+\\n\",\n",
    "      \"only showing top 20 rows\\n\",\n",
    "      \"\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"1596232\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 2,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"data.show()\\n\",\n",
    "    \"data.count()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# I developed two simple helper scripts to help display\\n\",\n",
    "    \"# the content of Params and ParamMaps in a more readable way\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def print_explainParams(cls, font_size=1):\\n\",\n",
    "    \"    \\\"\\\"\\\"Helper class for pretty printing MLlib parameters, \\n\",\n",
    "    \"    and similar output for use in Jupyter / IPython.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Usage example:\\n\",\n",
    "    \"    > print_explainParams(pyspark.ml.LogisticRegression)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Parameters\\n\",\n",
    "    \"    ----------\\n\",\n",
    "    \"    cls\\n\",\n",
    "    \"        input class (should be able to run the explainParams method)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    font_size : int\\n\",\n",
    "    \"        control displayed font size (default = 1)\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    title = f\\\"<h2>Parameters for: {str(cls)}</h2>\\\"\\n\",\n",
    "    \"    params = str(cls.explainParams()).split(\\\"\\\\n\\\")\\n\",\n",
    "    \"    html_body = \\\"\\\\n\\\".join([f'<h4>{p.replace(\\\":\\\", \\\"</h4> <p>\\\", 1)}</p>' for p in params])\\n\",\n",
    "    \"    display(HTML(f\\\"<font size='{font_size}'>{title} {html_body}</font>\\\"))\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def print_explainParamMap(cls, display_docs=True, font_size=1):\\n\",\n",
    "    \"    \\\"\\\"\\\"Helper class for pretty printing MLlib parameters, \\n\",\n",
    "    \"    and similar output for use in Jupyter / IPython.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Usage example:\\n\",\n",
    "    \"    > lr = pyspark.ml.LogisticRegressionModel\\n\",\n",
    "    \"    > print_explainParamMap(lr)\\n\",\n",
    "    \"    > print_explainParamMap(lr, False)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Parameters\\n\",\n",
    "    \"    ----------\\n\",\n",
    "    \"    cls\\n\",\n",
    "    \"        input class (should be able to run the explainParamMap method)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    font_size : int\\n\",\n",
    "    \"        control displayed font size (default = 1)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    display_docs : bool\\n\",\n",
    "    \"        toggles displaying the docs or not\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    title = f'<font size=\\\"{font_size}\\\"><h3>Parameter Map </h3>{model1}</font>'\\n\",\n",
    "    \"    param_map: dict = model1.extractParamMap()\\n\",\n",
    "    \"    html = []\\n\",\n",
    "    \"    if display_docs:\\n\",\n",
    "    \"        html.append(title)\\n\",\n",
    "    \"    for p in param_map.items():\\n\",\n",
    "    \"        param = p[0]\\n\",\n",
    "    \"        value = p[1]\\n\",\n",
    "    \"        if display_docs:\\n\",\n",
    "    \"            html.append(\\n\",\n",
    "    \"                f\\\"\\\"\\\"\\n\",\n",
    "    \"            <font size=\\\"{font_size}\\\"><h4>{param.name}</h4></font>\\n\",\n",
    "    \"            <p>\\n\",\n",
    "    \"                <font size=\\\"{font_size - 1}\\\">doc: <i>{param.doc}</i><br/>\\n\",\n",
    "    \"                value: </font><font size=\\\"{font_size + 1}\\\">{value}</font>\\n\",\n",
    "    \"            </p>\\n\",\n",
    "    \"            \\\"\\\"\\\"\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            html.append(\\n\",\n",
    "    \"                f'<li><font size=\\\"{font_size}\\\"><b>{param.name}:</b> {value}</font></li>'\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    display(HTML(f'{\\\"\\\".join(html)}'))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"We will split the data for Training, Validation, and Testing.  \\n\",\n",
    "    \"Since we have nearly 1.6 Million rows of data, we will split reserve 1% (15000-16000 records) each for validation and testing. That should be more than enough!\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 4,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"(training_data, validation_data, test_data) = data.randomSplit([0.98, 0.01, 0.01], seed=2020)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"CPU times: user 446 ms, sys: 177 ms, total: 622 ms\\n\",\n",
    "      \"Wall time: 4min 2s\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"%%time\\n\",\n",
    "    \"from pyspark.ml.feature import (\\n\",\n",
    "    \"    StopWordsRemover,\\n\",\n",
    "    \"    Tokenizer,\\n\",\n",
    "    \"    HashingTF,\\n\",\n",
    "    \"    IDF,\\n\",\n",
    "    \")\\n\",\n",
    "    \"from pyspark.ml.classification import LogisticRegression\\n\",\n",
    "    \"from pyspark.ml import Pipeline\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Tokenizer converts input string to lowercase and then splits it by white spaces.\\n\",\n",
    "    \"# https://spark.apache.org/docs/2.4.3/api/python/pyspark.ml.html#pyspark.ml.feature.Tokenizer\\n\",\n",
    "    \"# Params:\\n\",\n",
    "    \"tokenizer = Tokenizer(inputCol=\\\"text\\\", outputCol=\\\"words1\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# A feature transformer that filters out stop words from input.\\n\",\n",
    "    \"# https://spark.apache.org/docs/2.4.3/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover\\n\",\n",
    "    \"# Params:\\n\",\n",
    "    \"stopwords_remover = StopWordsRemover(\\n\",\n",
    "    \"    inputCol=\\\"words1\\\",\\n\",\n",
    "    \"    outputCol=\\\"words2\\\",\\n\",\n",
    "    \"    stopWords=StopWordsRemover.loadDefaultStopWords(\\\"english\\\"),  # English stopwords\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Maps a sequence of terms to their term frequencies using the hashing trick\\n\",\n",
    "    \"# https://spark.apache.org/docs/2.4.3/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF\\n\",\n",
    "    \"# Params:\\n\",\n",
    "    \"hashing_tf = HashingTF(inputCol=\\\"words2\\\", outputCol=\\\"term_frequency\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Compute the Inverse Document Frequency (IDF) given a collection of documents\\n\",\n",
    "    \"# https://spark.apache.org/docs/2.4.3/api/python/pyspark.ml.html#pyspark.ml.feature.IDF\\n\",\n",
    "    \"# Params:\\n\",\n",
    "    \"idf = IDF(\\n\",\n",
    "    \"    inputCol=\\\"term_frequency\\\",\\n\",\n",
    "    \"    outputCol=\\\"features\\\",\\n\",\n",
    "    \"    minDocFreq=5,  # minDocFreq: remove sparse terms\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"lr = LogisticRegression(labelCol=\\\"polarity\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"semantic_analysis_pipeline = Pipeline(\\n\",\n",
    "    \"    stages=[tokenizer, stopwords_remover, hashing_tf, idf, lr,]\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"semantic_analysis_model = semantic_analysis_pipeline.fit(training_data)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\\n\",\n",
    "      \"|                text|polarity|              words1|              words2|      term_frequency|            features|       rawPrediction|         probability|prediction|\\n\",\n",
    "      \"+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\\n\",\n",
    "      \"|  ' ' cute hug facey|     0.0|[', ', cute, hug,...|[', ', cute, hug,...|(262144,[23837,92...|(262144,[23837,92...|[8.11128118607166...|[0.48221068785989...|       4.0|\\n\",\n",
    "      \"|' ' i didn't mean...|     0.0|[', ', i, didn't,...|[', ', mean, anyt...|(262144,[991,3710...|(262144,[991,3710...|[9.44175669178636...|[0.92914747544691...|       0.0|\\n\",\n",
    "      \"|' ' it's a monkey...|     4.0|[', ', it's, a, m...|[', ', monkey, re...|(262144,[36200,60...|(262144,[36200,60...|[9.04815777628548...|[0.85752678521703...|       0.0|\\n\",\n",
    "      \"|' And she broke m...|     0.0|[', and, she, bro...|   [', broke, heart]|(262144,[92047,10...|(262144,[92047,10...|[9.14819224140936...|[0.88145221538831...|       0.0|\\n\",\n",
    "      \"|' But u were doin...|     0.0|[', but, u, were,...|[', u, something,...|(262144,[80680,92...|(262144,[80680,92...|[7.57272503959815...|[0.24108547631135...|       4.0|\\n\",\n",
    "      \"|' CRAZY IN LOVE I...|     0.0|[', crazy, in, lo...|[', crazy, love, ...|(262144,[92047,17...|(262144,[92047,17...|[7.08592657574225...|[0.10683060165750...|       4.0|\\n\",\n",
    "      \"|                 ' D|     0.0|              [', d]|              [', d]|(262144,[27526,92...|(262144,[27526,92...|[8.04040833985041...|[0.44820483516250...|       4.0|\\n\",\n",
    "      \"|' HEY YOU It's So...|     4.0|[', hey, you, it'...|[', hey, somaya, ...|(262144,[24918,26...|(262144,[24918,26...|[6.91039528474626...|[0.07670845739391...|       4.0|\\n\",\n",
    "      \"|' I am really tin...|     0.0|[', i, am, really...|[', really, tiny,...|(262144,[14,23837...|(262144,[14,23837...|[7.62059765644313...|[0.25805735863259...|       4.0|\\n\",\n",
    "      \"|' I believe in yo...|     4.0|[', i, believe, i...|[', believe, love...|(262144,[92047,11...|(262144,[92047,11...|[6.56332742833479...|[0.04007570533599...|       4.0|\\n\",\n",
    "      \"|   ' I cry for Heath|     0.0|[', i, cry, for, ...|     [', cry, heath]|(262144,[83897,92...|(262144,[83897,92...|[9.65900142141515...|[0.95382182358426...|       0.0|\\n\",\n",
    "      \"|' I haven't got a...|     0.0|[', i, haven't, g...|[', got, decent, ...|(262144,[31463,53...|(262144,[31463,53...|[8.08142282260806...|[0.46515281830123...|       4.0|\\n\",\n",
    "      \"|' I realy wana cr...|     0.0|[', i, realy, wan...|[', realy, wana, ...|(262144,[31950,43...|(262144,[31950,43...|[9.62582748590687...|[0.94940978334809...|       0.0|\\n\",\n",
    "      \"|' I totally forgo...|     0.0|[', i, totally, f...|[', totally, forg...|(262144,[89663,92...|(262144,[89663,92...|[8.75409860576438...|[0.77128935802592...|       0.0|\\n\",\n",
    "      \"|' I've been missi...|     0.0|[', i've, been, m...|[', missing, true...|(262144,[92047,20...|(262144,[92047,20...|[9.17198148388856...|[0.88617495125199...|       0.0|\\n\",\n",
    "      \"|' I've had my pho...|     0.0|[', i've, had, my...|[', phone, wif, w...|(262144,[31463,92...|(262144,[31463,92...|[8.81823602606339...|[0.79128458973670...|       0.0|\\n\",\n",
    "      \"| ' LAW OF ATTRACTION|     4.0|[', law, of, attr...|[', law, attraction]|(262144,[19003,92...|(262144,[19003,92...|[7.73912311522756...|[0.30745903679567...|       4.0|\\n\",\n",
    "      \"|' MY SUNGLASSSSSS...|     0.0|[', my, sunglasss...|[', sunglasssssss...|(262144,[92047,19...|(262144,[92047,19...|[8.07754599852299...|[0.46704952536319...|       4.0|\\n\",\n",
    "      \"|  ' Thestreetforce '|     4.0|[', thestreetforc...|[', thestreetforc...|(262144,[92047,21...|(262144,[92047,21...|[7.91469720212732...|[0.38669924921923...|       4.0|\\n\",\n",
    "      \"|' They're easier ...|     4.0|[', they're, easi...|[', easier, witho...|(262144,[8562,704...|(262144,[8562,704...|[8.67753459983778...|[0.74317857654588...|       0.0|\\n\",\n",
    "      \"+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\\n\",\n",
    "      \"only showing top 20 rows\\n\",\n",
    "      \"\\n\",\n",
    "      \"+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\\n\",\n",
    "      \"|                text|polarity|              words1|              words2|      term_frequency|            features|       rawPrediction|         probability|prediction|\\n\",\n",
    "      \"+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\\n\",\n",
    "      \"|' Today I woke up...|     0.0|[', today, i, wok...|[', today, woke, ...|(262144,[75373,92...|(262144,[75373,92...|[9.92923180694191...|[0.97252824118948...|       0.0|\\n\",\n",
    "      \"|' We'll miss ye f...|     4.0|[', we'll, miss, ...|[', miss, ye, sum...|(262144,[8443,218...|(262144,[8443,218...|[9.02852037099939...|[0.85148330437864...|       0.0|\\n\",\n",
    "      \"|'CRANKY' Oh oh ba...|     0.0|['cranky', oh, oh...|['cranky', oh, oh...|(262144,[23855,58...|(262144,[23855,58...|[8.23341128203349...|[0.54204891268949...|       0.0|\\n\",\n",
    "      \"|'Time Is An Illus...|     4.0|['time, is, an, i...|['time, illusion,...|(262144,[8443,535...|(262144,[8443,535...|[6.52119977522997...|[0.03691771757296...|       4.0|\\n\",\n",
    "      \"|'s boyfriend had ...|     0.0|['s, boyfriend, h...|['s, boyfriend, w...|(262144,[9168,343...|(262144,[9168,343...|[9.32512286439600...|[0.91247655749896...|       0.0|\\n\",\n",
    "      \"|'s boyfriend is g...|     0.0|['s, boyfriend, i...|['s, boyfriend, g...|(262144,[81948,17...|(262144,[81948,17...|[8.77030230666489...|[0.77657760254061...|       0.0|\\n\",\n",
    "      \"|A Congrats and ma...|     4.0|[a, congrats, and...|[congrats, many, ...|(262144,[91765,22...|(262144,[91765,22...|[6.34688065758123...|[0.02665079726518...|       4.0|\\n\",\n",
    "      \"|            A I know|     0.0|        [a, i, know]|              [know]|(262144,[140931],...|(262144,[140931],...|[8.00628552780356...|[0.43166342542476...|       4.0|\\n\",\n",
    "      \"|A Jonah more hugs...|     0.0|[a, jonah, more, ...|[jonah, hugs, ado...|(262144,[55882,10...|(262144,[55882,10...|[9.09215071332551...|[0.86867615562624...|       0.0|\\n\",\n",
    "      \"|A brought me BJ's...|     4.0|[a, brought, me, ...|[brought, bj's, p...|(262144,[15723,27...|(262144,[15723,27...|[8.13084072768372...|[0.49207505866175...|       4.0|\\n\",\n",
    "      \"|A central lesson ...|     0.0|[a, central, less...|[central, lesson,...|(262144,[27505,60...|(262144,[27505,60...|[8.13884294971551...|[0.49223180819948...|       4.0|\\n\",\n",
    "      \"|A colleague from ...|     0.0|[a, colleague, fr...|[colleague, thail...|(262144,[5381,912...|(262144,[5381,912...|[10.7965081679948...|[0.99496048233625...|       0.0|\\n\",\n",
    "      \"|A crap Thanks Sat...|     0.0|[a, crap, thanks,...|[crap, thanks, sa...|(262144,[25328,66...|(262144,[25328,66...|[8.23714673343472...|[0.54266679078946...|       0.0|\\n\",\n",
    "      \"|A few new swine c...|     0.0|[a, few, new, swi...|[new, swine, case...|(262144,[29945,12...|(262144,[29945,12...|[8.75255984695621...|[0.77043278880890...|       0.0|\\n\",\n",
    "      \"|   A froggy surprise|     4.0|[a, froggy, surpr...|  [froggy, surprise]|(262144,[188966,2...|(262144,[188966,2...|[7.60808575035167...|[0.25485225165812...|       4.0|\\n\",\n",
    "      \"|A jay kar just ga...|     4.0|[a, jay, kar, jus...|[jay, kar, gave, ...|(262144,[30444,42...|(262144,[30444,42...|[8.07539717435927...|[0.46363284720596...|       4.0|\\n\",\n",
    "      \"|A little buffalo ...|     4.0|[a, little, buffa...|[little, buffalo,...|(262144,[44587,15...|(262144,[44587,15...|[7.44010958630125...|[0.19545400003010...|       4.0|\\n\",\n",
    "      \"|A little wake up ...|     4.0|[a, little, wake,...|[little, wake, mu...|(262144,[13781,63...|(262144,[13781,63...|[7.59823341136554...|[0.25004652068055...|       4.0|\\n\",\n",
    "      \"|A neurologic inte...|     4.0|[a, neurologic, i...|[neurologic, inte...|(262144,[50660,96...|(262144,[50660,96...|[7.68203656158851...|[0.28035561927439...|       4.0|\\n\",\n",
    "      \"|A new day a new l...|     0.0|[a, new, day, a, ...|[new, day, new, l...|(262144,[13957,29...|(262144,[13957,29...|[9.06724966865572...|[0.86256698250418...|       0.0|\\n\",\n",
    "      \"+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\\n\",\n",
    "      \"only showing top 20 rows\\n\",\n",
    "      \"\\n\",\n",
    "      \"+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\\n\",\n",
    "      \"|                text|polarity|              words1|              words2|      term_frequency|            features|       rawPrediction|         probability|prediction|\\n\",\n",
    "      \"+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\\n\",\n",
    "      \"|'I wanna love you...|     0.0|['i, wanna, love,...|['i, wanna, love,...|(262144,[1576,208...|(262144,[1576,208...|[8.75780038219738...|[0.76986477958864...|       0.0|\\n\",\n",
    "      \"|'Property photogr...|     0.0|['property, photo...|['property, photo...|(262144,[8379,341...|(262144,[8379,341...|[8.76556968987144...|[0.77370680228484...|       0.0|\\n\",\n",
    "      \"|'Seven Pounds' co...|     4.0|['seven, pounds',...|['seven, pounds',...|(262144,[15554,26...|(262144,[15554,26...|[8.22303146567468...|[0.53589074910828...|       0.0|\\n\",\n",
    "      \"|                   A|     0.0|                 [a]|                  []|      (262144,[],[])|      (262144,[],[])|[8.00495230315631...|[0.43148151327643...|       4.0|\\n\",\n",
    "      \"|A Lenny We'll mis...|     0.0|[a, lenny, we'll,...|[lenny, miss, cra...|(262144,[74200,16...|(262144,[74200,16...|[9.91260385814582...|[0.97165352699665...|       0.0|\\n\",\n",
    "      \"|A M goin to sleep...|     4.0|[a, m, goin, to, ...|[m, goin, sleep, ...|(262144,[18910,31...|(262144,[18910,31...|[8.39174307940308...|[0.61777079496130...|       0.0|\\n\",\n",
    "      \"|A little sad pret...|     0.0|[a, little, sad, ...|[little, sad, pre...|(262144,[49185,61...|(262144,[49185,61...|[10.8675521756116...|[0.99565629041615...|       0.0|\\n\",\n",
    "      \"|A little sore fro...|     0.0|[a, little, sore,...|[little, sore, fo...|(262144,[126368,1...|(262144,[126368,1...|[8.64863308749478...|[0.73197043135156...|       0.0|\\n\",\n",
    "      \"|A pleasant evenin...|     4.0|[a, pleasant, eve...|[pleasant, evenin...|(262144,[12531,13...|(262144,[12531,13...|[6.96905296149435...|[0.08537679789726...|       4.0|\\n\",\n",
    "      \"|A sad day for Bri...|     0.0|[a, sad, day, for...|[sad, day, britis...|(262144,[13957,21...|(262144,[13957,21...|[9.52608126163851...|[0.94048678800376...|       0.0|\\n\",\n",
    "      \"|A thanks for cari...|     4.0|[a, thanks, for, ...|[thanks, caring, ...|(262144,[14,30781...|(262144,[14,30781...|[7.00549695727181...|[0.09250895812199...|       4.0|\\n\",\n",
    "      \"|A thousand miles ...|     0.0|[a, thousand, mil...|[thousand, miles,...|(262144,[17004,12...|(262144,[17004,12...|[7.32961185159772...|[0.16298191843973...|       4.0|\\n\",\n",
    "      \"|A very relaxed ev...|     0.0|[a, very, relaxed...|[relaxed, evening...|(262144,[11951,13...|(262144,[11951,13...|[7.95728517686299...|[0.40475296340018...|       4.0|\\n\",\n",
    "      \"|A weekend full of...|     0.0|[a, weekend, full...|[weekend, full, e...|(262144,[66452,96...|(262144,[66452,96...|[8.97475409726775...|[0.83954266777521...|       0.0|\\n\",\n",
    "      \"|A'm loving Tyra B...|     4.0|[a'm, loving, tyr...|[a'm, loving, tyr...|(262144,[31463,66...|(262144,[31463,66...|[7.84706061919994...|[0.35377197172940...|       4.0|\\n\",\n",
    "      \"|AAAAHHHHHHHHH Tmr...|     0.0|[aaaahhhhhhhhh, t...|[aaaahhhhhhhhh, t...|(262144,[52505,94...|(262144,[52505,94...|[8.44679675654815...|[0.64534171874674...|       0.0|\\n\",\n",
    "      \"|              AGREED|     4.0|            [agreed]|            [agreed]|(262144,[222069],...|(262144,[222069],...|[7.79738461718933...|[0.33337143853677...|       4.0|\\n\",\n",
    "      \"|AHH I'm too in lo...|     0.0|[ahh, i'm, too, i...|    [ahh, love, two]|(262144,[15664,29...|(262144,[15664,29...|[7.49713133241175...|[0.21469220301255...|       4.0|\\n\",\n",
    "      \"|AHHH work Slow do...|     0.0|[ahhh, work, slow...|[ahhh, work, slow...|(262144,[24792,27...|(262144,[24792,27...|[9.16996230060374...|[0.88523054462484...|       0.0|\\n\",\n",
    "      \"|AHHHH EVERYTHINGS...|     0.0|[ahhhh, everythin...|[ahhhh, everythin...|(262144,[32890,40...|(262144,[32890,40...|[11.0525097180845...|[0.99695915702475...|       0.0|\\n\",\n",
    "      \"+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\\n\",\n",
    "      \"only showing top 20 rows\\n\",\n",
    "      \"\\n\",\n",
    "      \"CPU times: user 250 ms, sys: 125 ms, total: 376 ms\\n\",\n",
    "      \"Wall time: 4.34 s\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"%%time\\n\",\n",
    "    \"trained_df = semantic_analysis_model.transform(training_data)\\n\",\n",
    "    \"val_df = semantic_analysis_model.transform(validation_data)\\n\",\n",
    "    \"test_df = semantic_analysis_model.transform(test_data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"trained_df.show()\\n\",\n",
    "    \"val_df.show()\\n\",\n",
    "    \"test_df.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 7,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# StopWordsRemover.loadDefaultStopWords(\\\"english\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 8,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Validation Data:\\n\",\n",
    "      \"Accuracy: 77.51357%\\n\",\n",
    "      \"Testing Data:\\n\",\n",
    "      \"Accuracy: 77.04075%\\n\",\n",
    "      \"CPU times: user 12.1 ms, sys: 10.1 ms, total: 22.2 ms\\n\",\n",
    "      \"Wall time: 10.6 s\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"%%time\\n\",\n",
    "    \"from pyspark.ml.evaluation import RegressionEvaluator\\n\",\n",
    "    \"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\\n\",\n",
    "    \"\\n\",\n",
    "    \"evaluator = MulticlassClassificationEvaluator(labelCol=\\\"polarity\\\", metricName=\\\"accuracy\\\")\\n\",\n",
    "    \"accuracy_val = evaluator.evaluate(val_df)\\n\",\n",
    "    \"accuracy_test = evaluator.evaluate(test_df)\\n\",\n",
    "    \"print(\\\"Validation Data:\\\")\\n\",\n",
    "    \"print(f\\\"Accuracy: {accuracy_val*100:.5f}%\\\")\\n\",\n",
    "    \"print(\\\"Testing Data:\\\")\\n\",\n",
    "    \"print(f\\\"Accuracy: {accuracy_test*100:.5f}%\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 9,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"final_model = semantic_analysis_pipeline.fit(data)\\n\",\n",
    "    \"final_model.save(MODEL_PATH)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.7.3\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
